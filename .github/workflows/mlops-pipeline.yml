name: MLOps CI

on:
  push:
    branches: [ main ]
    # Release tags like v1.2.3
    tags: [ 'v*.*.*' ]
  pull_request:
    branches: [ main ]

  # Manual runs are useful for debugging / demos.
  workflow_dispatch:

#
# Execution flow (lego-style, simple & predictable):
# 1) data-processing  -> produces processed data + preprocessor (artifacts)
# 2) model-training   -> consumes processed data, trains model (artifact)
# 3) build-and-publish-> consumes artifacts, builds and optionally pushes Docker image
#
env:
  PYTHON_VERSION: "3.11"
  MLFLOW_PORT: "5555" # Keep consistent with local MLflow usage

jobs:
  data-processing:
    name: Data processing + feature engineering
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip
          cache-dependency-path: requirements.txt

      - name: Verify required inputs exist
        run: |
          test -f data/raw/housing_data.csv

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install -r requirements.txt

      - name: Process data (raw -> cleaned)
        # Inputs/outputs are explicit so juniors can follow the lineage.
        run: |
          python src/data/run_processing.py \
            --input data/raw/housing_data.csv \
            --output data/processed/cleaned_house_data.csv

      - name: Engineer features (cleaned -> featured + preprocessor)
        run: |
          mkdir -p models/trained
          python src/features/engineer.py \
            --input data/processed/cleaned_house_data.csv \
            --output data/processed/featured_house_data.csv \
            --preprocessor models/trained/preprocessor.pkl

      - name: Upload featured dataset
        uses: actions/upload-artifact@v4
        with:
          name: processed-data
          path: data/processed/featured_house_data.csv

      - name: Upload preprocessor artifact
        uses: actions/upload-artifact@v4
        with:
          name: preprocessor
          path: models/trained/preprocessor.pkl

  model-training:
    name: Model training (MLflow tracked)
    needs: data-processing
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip
          cache-dependency-path: requirements.txt

      - name: Verify required inputs exist
        run: |
          test -f configs/model_config.yaml

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install -r requirements.txt

      - name: Download featured dataset
        uses: actions/download-artifact@v4
        with:
          name: processed-data
          path: data/processed/

      - name: Start MLflow tracking server (local to runner)
        run: |
          docker pull ghcr.io/mlflow/mlflow:latest
          docker run -d -p ${{ env.MLFLOW_PORT }}:5000 --name mlflow-server \
            ghcr.io/mlflow/mlflow:latest \
            mlflow server \
              --host 0.0.0.0 \
              --port 5000 \
              --backend-store-uri sqlite:///mlflow.db \
              --default-artifact-root /tmp/mlruns

      - name: Wait for MLflow to start
        run: |
          for i in {1..20}; do
            if curl -fsS "http://localhost:${{ env.MLFLOW_PORT }}/" >/dev/null; then
              echo "MLflow is up."
              exit 0
            fi
            sleep 2
          done
          echo "MLflow did not become ready in time."
          docker logs mlflow-server || true
          exit 1

      - name: Train model (featured -> model artifact)
        run: |
          mkdir -p models/trained
          python src/models/train_model.py \
            --config configs/model_config.yaml \
            --data data/processed/featured_house_data.csv \
            --models-dir models \
            --mlflow-tracking-uri "http://localhost:${{ env.MLFLOW_PORT }}"

      - name: Upload trained model artifact
        uses: actions/upload-artifact@v4
        with:
          name: trained-model
          path: models/trained/

      - name: Clean up MLflow
        if: always()
        run: |
          docker stop mlflow-server || true
          docker rm mlflow-server || true

  build-and-publish:
    name: Build & publish container image
    needs: model-training
    runs-on: ubuntu-latest
    # Never push images from PRs (secrets are unavailable for forks).
    if: ${{ github.event_name != 'pull_request' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download trained model
        uses: actions/download-artifact@v4
        with:
          name: trained-model
          path: models/trained/

      - name: Download preprocessor
        uses: actions/download-artifact@v4
        with:
          name: preprocessor
          path: models/trained/

      - name: Set up QEMU (multi-arch)
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          registry: docker.io
          username: ${{ vars.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: true
          tags: |
            docker.io/${{ vars.DOCKERHUB_USERNAME }}/house-price-model:latest
            docker.io/${{ vars.DOCKERHUB_USERNAME }}/house-price-model:${{ github.sha }}
          platforms: linux/amd64,linux/arm64

